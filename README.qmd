---
title: "Progress Report" 
author: "William & Sebastian" 
format: gfm 
editor: visual
---

## Research Question

-   What is the impact of air pollution caused by light rail stations on demographics in Phoenix and Mesa?

## Hypothesis

-   The Light Rail in Phoenix-Mesa should help alleviate the amount of PM2.5 being released into the air, helping decrease air quality diseases around the area. Phoenix-Mesa has seen an increase in population, it has had a 16.31% increase in population from 2004 to 2012, increasing further as the years progressed. this increase in population is enough to congest roads and increase the demand for power to provide to the citizens. All of this can increase the levels of PM2.5. the light rail will help alleviate the by delivering faster, community transportation that would decrease the number of cars in the streets. With fewer cars in the streets, roads can help decongest faster. So Overall, the impact of the light rail on air pollution in the Phoenix-Mesa region (Maricopa County) will increase as more people use the light rail, further reducing one of their biggest contributors of PM2.5 pollution.

## Data

```{r}
library("tidyverse") 
library("knitr") 
library("terra") 
library("maptiles")
```

-   Timeline of interest

    -   Given the time frame of the data we have access to from NASA, the only stations that existed then all opened on December 27th, 2008. We want to track the pollution in an equal timeframe before and after the opening, so we chose the timeframe of January 1st, 2004 to January 1st, 2012

-   Station locations

    -   We used a Google API key to collect the coordinates of each of these stations, manually collecting the few that Google didn't automatically find.

        ```{r}
        c<-read.csv("Coordinates.csv") %>%   
          select(Station, lat2, lon2) 
        kable(c)
        ```

-   Factors that impact PM2.5 in the city

    Many factors contribute to the PM2.5 level in the air in Maricopa County, this is the Phoenix-Mesa area we are focusing on. According to Maricopa's official website, some of the biggest contributors to PM2.5 pollution include wood burning, power plants, congested highways, construction sites, and unpaved roads. Using the same technique of acquiring their coordinates from Google API, we can make a table that contains some of the biggest contributors around this county, some more centralized in the city than others.

```{r}
sources <- read.csv("Poll_Coordinates.csv") %>%     
  filter(Source != "SOURCE Arizona") %>%     
  select(Source,lat2,lon2) 
kable(sources) 
```

## Plotting Stations

Once we had the coordinates, we could plot them out and create a map displaying the stations and the line they all service, as well as a circular buffer representing the station's area of effect.

```{r}
stations <- read.csv("Coordinates.csv") 
sources <- read.csv("Poll_Coordinates.csv")  
df<-stations |>   select(lon2, lat2)  
df2<-sources |>   select(lon2, lat2)  
#converts df into a spatvector 
x <- vect(df, geom=c("lon2", "lat2"), crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs ") 
y <- vect(df2, geom=c("lon2", "lat2"), crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs ") 
#these are the stations  plot(x) #these are our sources of Pollution in Maricopa County 
plot(y) 
```

This would then put buffers over our stations with a 1000m radius. We get a more complete map by combining a map background, our points, and their station buffers.

```{r}
#create a 1 km (1000 meter) buffer (radius)  
pts_buffer<-buffer(x, width = 400) 
plot(pts_buffer)   
#approximate size of the background 
extent<-buffer(x, width = 400)  
bg <- get_tiles(ext(extent))  
plot(bg)  
#plotting the Stations and their buffers,as well as sources of pollution 
lines(pts_buffer) 
points(x, col = "red") 
points(y, col = "blue") 
outfile <- "buffer.shp" 
writeVector(pts_buffer, outfile, overwrite=TRUE) 
```

## **Gathering Meteorology Data**

To check the effects of pollution from sources around the city, whether it comes from stations, power plants, and highways or the effects of policies that might affect these levels of pollution, we need meteorological data. we will be using data from NASA, through its Land Data Assimilation System (LDAS) and its global counterpart (GLDAS). These data can be gathered from NASA's website. We downloaded all relevant data into a Google Drive folder, ranging from the dates of 01/01/2000 to 30/12/20214. This already covers our original date range of 01/01/2000 to 01/01/2012.

The code below opens this data and loops through its files to only get the relevant dates. Next, it uses the buffer zones around the train stations to only put the data from these areas. Lastly, it puts it all into a CSV file.

```{r}
files<-dir("G:/Shared drives/2024 FIRE Light Rail/DATA/GLDAS/")

output<-c()

for(i in 1822:4384){
  r<-rast(paste0("G:/Shared drives/2024 FIRE Light Rail/DATA/GLDAS/", files[i]))
  
  sta<-vect("buffer.shp")
  
  #crops raster to contain only buffers around stations
  int<-crop(r, sta,
            snap="in",
            mask=TRUE)
  
  #convert cropped raster into dataframe and fine average value
  metdf<-terra::extract(int, sta, fun="mean", na.rm=TRUE)  %>%
    summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE))) %>%
    select(-ID)
  
  metdf$date<-files[i]
  output<-rbind(output, metdf)
  print(files[i])
}
 
write.csv(output, "met_data.csv", row.names=F)
```

## **Gathering PM2.5 Data**

Similarly to Meteorology data, we need to also gather PM2.5 to see the changes these stations have on this type of pollution. we focus on the same time frame as meteorology data. Our data comes from NASA. We downloaded the data into another Google Drive folder. we then use a nested for-loop to go through each folder (which are months), and go through each day (they are TIF files containing satellite images of the US). The buffers we used for the meteorology data are also used again to focus on those parts of the map. We put all these daily files into a folder that will be used later on.

```{r}
pts_buffer <- vect("buffer.shp")

path<-"G:/Shared drives/2024 FIRE Light Rail/DATA/PM25/"
months<-dir(path)
# for each month
for (m in 1:length(months)) {
  print(months[m])
  days<-dir(paste0(path,months[m]))
  
  # for each day in this month
  days_output<-c()
  for (d in 1:length(days)) {
    print(days[d])
    
    #read tif file
    r<-rast(paste0(path, months[m], "/", days[d]))
    
    #changes the crs system
    buffer_project<-terra::project(pts_buffer,  crs(r))
    
    #pts_buffer is the buffer around stations
    #crops raster to contain only buffers around stations
    int<-crop(r, buffer_project,
              snap="in",
              mask=TRUE)
    
    #convert cropped raster into dataframe and fine average value
    cntrl_df<-terra::extract(int, buffer_project, fun="mean", na.rm=TRUE)
    
    #rename columns
    names(cntrl_df)<-c("city_num","pm25")
    
    #create a dataframe date, shape index, and pm25
    output <- as.data.frame(c("date"=days[d], cntrl_df))
    
    #combine output with previous looop
    days_output<-rbind(days_output, output)
   
    
  }
  write.csv(days_output, 
            paste0("PM25_daily/lr_centroid_",
                   months[m],
                   ".csv")
            , row.names = F)
  
}
```

## **Combining all data gathered**

Now that we have station locations, sources of pollution locations, meteorology data, and PM2.5 data, we combine all of these into one single data frame. Holiday dates were added and compared to dates on the time frame to verify if those dates were holidays. all of the data are then combined into a data frame using merge() and outputted into a CSV file.

```{r}
setwd("PM25_daily")
file_list <- list.files()

dataset<-data.frame()

for (file in file_list){
    temp_dataset <-read.csv(file)
    dataset<-rbind(dataset, temp_dataset)
    rm(temp_dataset)
}
setwd("..")
stations<-read.csv("Coordinates.csv")
merge1<-merge(dataset,stations, by.x = "city_num", by.y = "X")
newdates <- gsub("(....)(..)(..)(\\.tif)", "\\1-\\2-\\3", merge1$date)
merge1$date <- as.Date(newdates)
holidays <- read.csv("major_holidays_2000_2025.csv")
is_holiday <- newdates %in% holidays$date
merge1$is_holiday <- is_holiday
merge1$dow <- weekdays(merge1$date)
merge1$month <- format(merge1$date, "%m")
dfm<-read.csv("met_data.csv")
newdates2 <- as.Date(gsub("(....)(..)(..)(\\.020\\.nc4)", "\\1-\\2-\\3", dfm$date))
dfm$date <- newdates2
merge2<-merge(merge1, dfm, by.x="date", by.y="date")

write.csv(merge2, "Big_Data.csv")

```

## Modeling and using DB-OLS regression

we now want to create a model that would give us a coefficient that we can use to tell the relationship between the pollution levels and the metro stations opening. we will create our dates for the start of the analysis to the end of it, the opening date of the stations, when construction began for the stations, and a policy that was enacted around that same time. df2 has our initial big data that was combined above, but it removes unnecessary columns that wont be used on the regression.

```{r}
df<-read.csv("Big_Data.csv")
df2<-df %>% mutate(date=as.Date(date, format='%Y-%m-%d'))
df2 <- subset(df2, select = -c(V1, V2, V3, lon, lat, address, X, lat2, lon2, Station))

#period of analysis

startdate<-as.Date("2004-12-01", format='%Y-%m-%d')
enddate<-as.Date("2012-12-01", format='%Y-%m-%d')
opendate<-as.Date("2008-12-27", format='%Y-%m-%d')
conststart<-as.Date("2005-03-01", format='%Y-%m-%d')
poldate<-as.Date("2004-01-21", format='%Y-%m-%d')
```

We now add variables that will be used for the regression. we have a policy variable that would be a 1 when it is active and 0 when it is not. Like with with Metro open, it just states which dates the metro stations where open or not. other variables are temperature, wind, humidity and time variable.

```{r}
df3<-df2 %>%
  filter(date>=startdate & date<=enddate) %>%
  mutate(policy=ifelse(date>=poldate, 1, 0)) %>%
  mutate(MetroOpen=ifelse(date>=opendate, 1, 0)) %>%
  mutate(dow=wday(date)) %>%
  mutate(construction=ifelse(date>conststart & date<opendate, 
                             1, 0)) %>%
  #Create P(t) variables
  mutate(t=as.numeric(date-startdate)) %>%
  mutate(t2=t^2, t3=t^3, t4=t^4) %>%
  #Create lagged values (do this for temperature, humidity, and wind speed)
  arrange(city_num, date) %>%
  group_by(city_num) %>%
  mutate(lag_temp=lag(Tair_f_tavg)) %>%
  mutate(lag_temp2=lag_temp^2, lag_temp3=lag_temp^3, lag_temp4=lag_temp^4) %>%
  mutate(lag_wind=lag(Wind_f_tavg)) %>%
  mutate(lag_wind2=lag_wind^2, lag_wind3=lag_wind^3, lag_wind4=lag_wind^4) %>%
  mutate(lag_hum=lag(Qair_f_tavg)) %>%
  mutate(lag_hum2=lag_hum^2, lag_hum3=lag_hum^3, lag_hum4=lag_hum^4) %>%
  mutate(MetroTime=MetroOpen*t, MetroTime2=MetroOpen*t2, MetroTime3=MetroOpen*t3, MetroTime4=MetroOpen*t4)
```

Our first simple regression is just to see the effect on PM2.5 when the metro is open. this is not a good model as it only considers one variable. we would need to add more variables to see if any hidden variables might be affecting the levels of pollution in our areas. so we begin to add more regression to see how the variable "Metro Open" changes with each regression and added variable.

```{r}
#simple regression
summary(m1<-lm(log(pm25) ~ MetroOpen , data=df3))

summary(m1<-lm(log(pm25) ~ MetroOpen + as.factor(dow) + as.factor(month), data=df3))

summary(m1<-lm(log(pm25) ~ MetroOpen + as.factor(dow) + as.factor(month) +lag_temp+lag_wind+lag_hum+construction +policy, data=df3))
```
